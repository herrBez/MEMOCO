\section{Second Assignment}
\label{sec:problemII}
The second assignment consisted in the implementation of a solution for the problem
using a metaheuristic approach.
I chose to use a population based solution, because I think that I can reuse this algorithm
in the future, because of its great flexibility.
\paragraph{Encoding of the individuals.} In order to encode the individuals I used the suggested
encoding, i.e. an array of dimension $N+1$ containing a sequence of holes index, e.g.
$<0\ 1\ \dots\ N\ 0>$.

\paragraph{Initial Population.} The initial population is formed by $S$ individuals.
In order to diversify the population, fast all the individuals are chosen randomly, but
the increasing sequence $<0\ 1\ \dots\ N\ 0>$ and the decreasing sequence $<0\ N\ \dots\ 1\ 0>$
are always granted, in order to have a couple of solutions with big Hamming distance. 


\paragraph{Fitness Function.} For simplicity the fitness function is the inverse of the
objective value.

\paragraph{Selection.} Although I had implemented the three methods showed in class, I used the \emph{Montecarlo} approach
for the selection during the increasing phase, because this method leads to a very fast convergence; instead in the diversification phase the \emph{N-Tournament} (with size $R$) approach 
was preferred , because it
mitigates the effects of the chosen population substitution strategy, that 
keeps only the best elements.

\paragraph{Combination.} For the combination I used the ordering crossover with two randomly
chosen cutting points by adopting the
\verb|std::map<int,int>| data-structure, defined in the C++ header \verb|<map>|.
This data-structure has two desirable features:
\begin{itemize}
	\item It is very efficient because it is 
	implemented as a RB-Tree\footnote{http://en.cppreference.com/w/cpp/container/map}
	\item More important, it is sorted by key, i.e. the first element of the pair.

\end{itemize} 
The latter characteristic permits an easy implementation of the ordering crossover.

The efficiency of the Combination cannot be neglected, because this operator is used $R$ times in each
iteration and in the worst case the difference between the two randomly chosen cutting point can be very close to $N$.
The usage inside the algorithm of this data-structure can be viewed in the file \verb|TSPSolverGA.h|. 

\paragraph{Intensification and Diversification} The algorithm starts with an intensification phase. 
After 500 non improving iterations, a diversification phase follows during which
\begin{itemize}
	\item The mutation is performed with a greater probability;
	\item The selection is done according to the n-tournament strategy;
	\item The values are substituted  randomly but the algorithm keeps always the best two elements of the population.
\end{itemize}  

\paragraph{Population Substitution.} During each iteration $R$ new individuals are created,
and attached to the end of the population. Therefore
the cardinality of the new population is  $S+R$.
Afterward the population is sorted in descending order according to the fitness value. 

Since $R$ is smaller than $S$ and the population
at the start of the iteration is sorted, \emph{insertion sort}
is used. This works very efficiently (linearly) with inputs that are almost sorted.
Instead the standard sort algorithm is (usually) \emph{quicksort}, which performs extremely poor (in quadratic time) with almost sorted vectors.

After the sort operation only the $S$ best elements are kept, discarding the last $R$ elements.
Instead during the diversification phase, this strategy is substituted by a random choice of elements to keep, but it is always granted
that the best two elements are kept in the population.

\paragraph{Stopping Criterion.} The algorithm terminates when it reaches 500 non improving iterations after
the alternation of two intensification and two diversification phases.
The drawback of this approach compared to a time based stop is that it is not predictable when
the algorithm will finish. In addition sometimes the improvements are so low that it is not worth to repeat other 500 iterations.

% but
%otherwise it could happen that a few more iteration would have produce better result.
%The advantage w.r.t. the stop after a while is that it is not arbitrary.

\paragraph{Finalization}
At the end of the algorithm the best element\footnote{It corresponds to the first element of the population, because it is sorted.} of the population is returned. But first it is trained with a random greedy local search, that render it (possibly) even better.



\subsection{Parameter calibration}
Since the factors on which the solution depends are many - among others the probability of the mutation, selection method, number of non improving iterations, size of population, how the initial solution are created etc - I fixed the majority of them after a number of tries.
In the section \ref{sec:II:performance} the impact of the size of the population on the quality and performance of the algorithm will be 
discussed.
